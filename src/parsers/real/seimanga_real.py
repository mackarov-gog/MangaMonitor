import re
import os
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict
from parsers.base_parser import BaseParser





class SeiMangaParser(BaseParser):
    BASE_URL = "https://1.seimanga.me"

    async def _fetch_html(self, url: str) -> str:
        headers = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers) as resp:
                return await resp.text()

    # ðŸ”Ž ÐŸÐ¾Ð¸ÑÐº Ð¼Ð°Ð½Ð³Ð¸
    async def search(self, query: str) -> List[Dict]:
        """
        Ð˜Ñ‰ÐµÑ‚ Ð¼Ð°Ð½Ð³Ñƒ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ.
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº [{title, url}].
        """
        search_url = f"{self.BASE_URL}/list?title={query}"
        html = await self._fetch_html(search_url)
        soup = BeautifulSoup(html, "html.parser")

        results = []
        for item in soup.select("div.item"):
            link = item.select_one("a.cover")
            title = item.select_one("div.title")
            if link and title:
                results.append({
                    "title": title.get_text(strip=True),
                    "url": self.BASE_URL + link["href"]
                })
        return results

    # ðŸ“˜ Ð˜Ð½Ñ„Ð¾ Ð¾ Ð¼Ð°Ð½Ð³Ðµ
    async def get_manga_info(self, url: str) -> Dict:
        """
        Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¸Ð½Ñ„Ð¾ Ð¾ Ð¼Ð°Ð½Ð³Ðµ + ÑÐ¿Ð¸ÑÐ¾Ðº Ð³Ð»Ð°Ð².
        """
        html = await self._fetch_html(url)
        soup = BeautifulSoup(html, "html.parser")

        title = soup.select_one("h1.names").get_text(strip=True)
        author = soup.select_one("span.author a")
        description = soup.select_one("div.description")

        chapters = []
        for row in soup.select("tr.item-row"):
            ch_link = row.select_one("a.chapter-link")
            ch_date = row.select_one("td.date")
            if ch_link:
                chapters.append({
                    "title": ch_link.get_text(strip=True),
                    "url": self.BASE_URL + ch_link["href"],
                    "date": ch_date.get_text(strip=True) if ch_date else None
                })

        return {
            "title": title,
            "author": author.get_text(strip=True) if author else None,
            "genres": [g.get_text(strip=True) for g in soup.select("span.genre")],
            "description": description.get_text(strip=True) if description else "",
            "chapters": chapters
        }

    # ðŸ“„ Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð³Ð»Ð°Ð²
    async def get_chapters(self, manga_url: str) -> List[Dict]:
        info = await self.get_manga_info(manga_url)
        return info["chapters"]

    # ðŸ–¼ï¸ Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð³Ð»Ð°Ð²Ñ‹
    async def get_chapter_pages(self, chapter_url: str) -> List[str]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ñ‚ rm_h.readerInit(...) Ð¸ Ð´Ð¾ÑÑ‚Ð°Ñ‘Ñ‚ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸.
        """
        html = await self._fetch_html(chapter_url)
        matches = re.findall(r"\['(https://[^']+)','',\"([^\"]+)\"", html)

        image_urls = []
        for base, path in matches:
            full_url = base + path
            clean_url = full_url.split("?")[0]  # ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ query string
            image_urls.append(clean_url)

        return image_urls

    # ðŸ’¾ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð³Ð»Ð°Ð²Ñƒ
    async def download_chapter(self, chapter_url: str, out_dir: str) -> List[str]:
        os.makedirs(out_dir, exist_ok=True)
        image_urls = await self.get_chapter_pages(chapter_url)
        saved_files = []

        async with aiohttp.ClientSession() as session:
            for i, url in enumerate(image_urls, 1):
                async with session.get(url) as resp:
                    if resp.status == 200:
                        data = await resp.read()
                        ext = os.path.splitext(url)[1] or ".jpg"
                        filename = os.path.join(out_dir, f"{i}{ext}")
                        with open(filename, "wb") as f:
                            f.write(data)
                        saved_files.append(filename)
        return saved_files
